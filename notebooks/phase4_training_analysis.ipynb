{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Post-Training Analysis\n",
                "\n",
                "This notebook analyzes the trained Lead Scout Model to verify:\n",
                "1. **Loss Convergence** - Are train/val losses decreasing?\n",
                "2. **Evaluation Metrics** - Precision, Recall, F1-score\n",
                "3. **Attention Patterns** - Before vs After training comparison\n",
                "4. **Confusion Matrix** - False positives/negatives analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Setup\n",
                "# ============================================================\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
                "\n",
                "from src.tokenizer import SalesTokenizer\n",
                "from src.model.lead_scout import LeadScoutModel\n",
                "from src.data.dataset import LeadDataset\n",
                "from torch.utils.data import DataLoader, random_split\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"‚úÖ Imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Training History & Plot Loss Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Load Training History\n",
                "# ============================================================\n",
                "history_path = '../checkpoints/training_history.json'\n",
                "\n",
                "try:\n",
                "    with open(history_path, 'r') as f:\n",
                "        history = json.load(f)\n",
                "    print(f\"‚úÖ Loaded training history with {len(history['train_loss'])} epochs\")\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ö†Ô∏è No training history found. Run train.py first.\")\n",
                "    history = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: Plot Training Curves\n",
                "# ============================================================\n",
                "if history:\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    epochs = range(1, len(history['train_loss']) + 1)\n",
                "    \n",
                "    # Loss curve\n",
                "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
                "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
                "    axes[0, 0].set_xlabel('Epoch')\n",
                "    axes[0, 0].set_ylabel('Loss')\n",
                "    axes[0, 0].set_title('Loss Curve (Lower is Better)')\n",
                "    axes[0, 0].legend()\n",
                "    axes[0, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Check for convergence\n",
                "    if len(history['train_loss']) > 1:\n",
                "        loss_decrease = history['train_loss'][0] - history['train_loss'][-1]\n",
                "        if loss_decrease > 0:\n",
                "            axes[0, 0].annotate(f'Loss decreased by {loss_decrease:.4f}', \n",
                "                               xy=(0.5, 0.95), xycoords='axes fraction', \n",
                "                               fontsize=10, color='green')\n",
                "    \n",
                "    # Accuracy curve\n",
                "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
                "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
                "    axes[0, 1].set_xlabel('Epoch')\n",
                "    axes[0, 1].set_ylabel('Accuracy')\n",
                "    axes[0, 1].set_title('Accuracy Curve (Higher is Better)')\n",
                "    axes[0, 1].legend()\n",
                "    axes[0, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Precision/Recall curve\n",
                "    axes[1, 0].plot(epochs, history['train_precision'], 'b-', label='Train Precision', linewidth=2)\n",
                "    axes[1, 0].plot(epochs, history['val_precision'], 'r-', label='Val Precision', linewidth=2)\n",
                "    axes[1, 0].plot(epochs, history['train_recall'], 'b--', label='Train Recall', linewidth=2)\n",
                "    axes[1, 0].plot(epochs, history['val_recall'], 'r--', label='Val Recall', linewidth=2)\n",
                "    axes[1, 0].set_xlabel('Epoch')\n",
                "    axes[1, 0].set_ylabel('Score')\n",
                "    axes[1, 0].set_title('Precision & Recall Curves')\n",
                "    axes[1, 0].legend()\n",
                "    axes[1, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # F1 Score curve\n",
                "    axes[1, 1].plot(epochs, history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
                "    axes[1, 1].plot(epochs, history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
                "    axes[1, 1].set_xlabel('Epoch')\n",
                "    axes[1, 1].set_ylabel('F1 Score')\n",
                "    axes[1, 1].set_title('F1 Score Curve (Higher is Better)')\n",
                "    axes[1, 1].legend()\n",
                "    axes[1, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print final metrics\n",
                "    print(\"\\n=== Final Epoch Metrics ===\")\n",
                "    print(f\"Train - Loss: {history['train_loss'][-1]:.4f}, Acc: {history['train_acc'][-1]:.4f}, \"\n",
                "          f\"P: {history['train_precision'][-1]:.4f}, R: {history['train_recall'][-1]:.4f}, F1: {history['train_f1'][-1]:.4f}\")\n",
                "    print(f\"Val   - Loss: {history['val_loss'][-1]:.4f}, Acc: {history['val_acc'][-1]:.4f}, \"\n",
                "          f\"P: {history['val_precision'][-1]:.4f}, R: {history['val_recall'][-1]:.4f}, F1: {history['val_f1'][-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Trained Model & Evaluate on Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 4: Load Trained Model\n",
                "# ============================================================\n",
                "model_path = '../checkpoints/lead_scout_best.pth'\n",
                "\n",
                "# Initialize model\n",
                "model = LeadScoutModel(\n",
                "    vocab_size=17,\n",
                "    embed_dim=128,\n",
                "    num_heads=4,\n",
                "    num_layers=3,\n",
                "    dropout=0.1\n",
                ")\n",
                "\n",
                "try:\n",
                "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
                "    model.eval()\n",
                "    print(f\"‚úÖ Loaded trained model from {model_path}\")\n",
                "    trained_model_available = True\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ö†Ô∏è No trained model found. Run train.py first.\")\n",
                "    trained_model_available = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Prepare Validation Data\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    # Load dataset\n",
                "    dataset = LeadDataset('../data/leads_raw.csv')\n",
                "    \n",
                "    # Use same split as training (for fair comparison)\n",
                "    torch.manual_seed(42)  # Same seed as training\n",
                "    train_size = int(0.8 * len(dataset))\n",
                "    val_size = len(dataset) - train_size\n",
                "    _, val_dataset = random_split(dataset, [train_size, val_size])\n",
                "    \n",
                "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
                "    print(f\"‚úÖ Loaded {len(val_dataset)} validation samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Run Inference & Collect Predictions\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    all_probs = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for tokens, labels in val_loader:\n",
                "            outputs = model(tokens)\n",
                "            probs = outputs.squeeze()\n",
                "            preds = (outputs > 0.5).float().squeeze()\n",
                "            \n",
                "            all_probs.extend(probs.numpy().flatten())\n",
                "            all_preds.extend(preds.numpy().flatten())\n",
                "            all_labels.extend(labels.numpy().flatten())\n",
                "    \n",
                "    all_preds = np.array(all_preds)\n",
                "    all_labels = np.array(all_labels)\n",
                "    all_probs = np.array(all_probs)\n",
                "    \n",
                "    print(f\"‚úÖ Collected {len(all_preds)} predictions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Confusion Matrix & Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 7: Confusion Matrix\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    cm = confusion_matrix(all_labels, all_preds)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
                "                xticklabels=['No Reply', 'Reply'],\n",
                "                yticklabels=['No Reply', 'Reply'])\n",
                "    ax.set_xlabel('Predicted')\n",
                "    ax.set_ylabel('Actual')\n",
                "    ax.set_title('Confusion Matrix on Validation Set')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print classification report\n",
                "    print(\"\\n=== Classification Report ===\")\n",
                "    print(classification_report(all_labels, all_preds, \n",
                "                               target_names=['No Reply', 'Reply']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ROC Curve & AUC Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 8: ROC Curve\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
                "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
                "    ax.set_xlim([0.0, 1.0])\n",
                "    ax.set_ylim([0.0, 1.05])\n",
                "    ax.set_xlabel('False Positive Rate')\n",
                "    ax.set_ylabel('True Positive Rate')\n",
                "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nAUC Score: {roc_auc:.4f}\")\n",
                "    if roc_auc > 0.7:\n",
                "        print(\"‚úÖ Model has learned meaningful patterns (AUC > 0.7)\")\n",
                "    elif roc_auc > 0.5:\n",
                "        print(\"‚ö†Ô∏è Model is better than random, but could improve\")\n",
                "    else:\n",
                "        print(\"‚ùå Model is not better than random guessing\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Attention Pattern Analysis (Before vs After Training)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 9: Get Attention Weights from Trained Model\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    # Get a sample batch\n",
                "    sample_tokens, sample_labels = next(iter(val_loader))\n",
                "    sample_tokens = sample_tokens[:5]  # First 5 samples\n",
                "    sample_labels = sample_labels[:5]\n",
                "    \n",
                "    # Hook to capture attention weights\n",
                "    attention_weights = []\n",
                "    \n",
                "    def hook_fn(module, input, output):\n",
                "        # output is (attn_output, attn_weights)\n",
                "        attention_weights.append(output[1].detach())\n",
                "    \n",
                "    # Register hook on first transformer block's attention layer\n",
                "    hook = model.transformer_blocks[0].attention.register_forward_hook(hook_fn)\n",
                "    \n",
                "    # Forward pass\n",
                "    with torch.no_grad():\n",
                "        _ = model(sample_tokens)\n",
                "    \n",
                "    # Remove hook\n",
                "    hook.remove()\n",
                "    \n",
                "    trained_attn = attention_weights[0]\n",
                "    print(f\"‚úÖ Captured attention weights: {trained_attn.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 10: Get Attention Weights from Untrained Model\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    # Create fresh untrained model\n",
                "    untrained_model = LeadScoutModel(\n",
                "        vocab_size=17,\n",
                "        embed_dim=128,\n",
                "        num_heads=4,\n",
                "        num_layers=3,\n",
                "        dropout=0.1\n",
                "    )\n",
                "    untrained_model.eval()\n",
                "    \n",
                "    # Hook to capture attention weights\n",
                "    untrained_attention_weights = []\n",
                "    \n",
                "    def untrained_hook_fn(module, input, output):\n",
                "        untrained_attention_weights.append(output[1].detach())\n",
                "    \n",
                "    # Register hook\n",
                "    untrained_hook = untrained_model.transformer_blocks[0].attention.register_forward_hook(untrained_hook_fn)\n",
                "    \n",
                "    # Forward pass with same tokens\n",
                "    with torch.no_grad():\n",
                "        _ = untrained_model(sample_tokens)\n",
                "    \n",
                "    # Remove hook\n",
                "    untrained_hook.remove()\n",
                "    \n",
                "    untrained_attn = untrained_attention_weights[0]\n",
                "    print(f\"‚úÖ Captured untrained attention weights: {untrained_attn.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 11: Compare Attention Patterns\n",
                "# ============================================================\n",
                "if trained_model_available:\n",
                "    # Get token names for visualization\n",
                "    tokenizer = SalesTokenizer()\n",
                "    \n",
                "    # Visualize first sample\n",
                "    sample_idx = 0\n",
                "    token_ids = sample_tokens[sample_idx].numpy()\n",
                "    token_names = [tokenizer.id_to_token.get(int(tid), f'[{tid}]') for tid in token_ids if tid != 0]\n",
                "    seq_len = len(token_names)\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "    \n",
                "    # Untrained attention\n",
                "    attn_untrained = untrained_attn[sample_idx, :seq_len, :seq_len].numpy()\n",
                "    sns.heatmap(attn_untrained, ax=axes[0], cmap='viridis', \n",
                "                xticklabels=token_names, yticklabels=token_names,\n",
                "                annot=True, fmt='.2f', cbar=True)\n",
                "    axes[0].set_title('BEFORE Training (Random Weights)', fontsize=14)\n",
                "    axes[0].set_xlabel('Key (Attended To)')\n",
                "    axes[0].set_ylabel('Query (Attending)')\n",
                "    \n",
                "    # Trained attention\n",
                "    attn_trained = trained_attn[sample_idx, :seq_len, :seq_len].numpy()\n",
                "    sns.heatmap(attn_trained, ax=axes[1], cmap='viridis', \n",
                "                xticklabels=token_names, yticklabels=token_names,\n",
                "                annot=True, fmt='.2f', cbar=True)\n",
                "    axes[1].set_title('AFTER Training (Learned Weights)', fontsize=14)\n",
                "    axes[1].set_xlabel('Key (Attended To)')\n",
                "    axes[1].set_ylabel('Query (Attending)')\n",
                "    \n",
                "    label_text = 'Reply' if sample_labels[sample_idx].item() == 1 else 'No Reply'\n",
                "    fig.suptitle(f'Attention Pattern Comparison (Label: {label_text})', fontsize=16, y=1.02)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüìä Key Observations:\")\n",
                "    print(\"- BEFORE: Attention is often uniform or focused on special tokens ([START], [END])\")\n",
                "    print(\"- AFTER: Attention should show more meaningful patterns between features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary & Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 12: Training Quality Summary\n",
                "# ============================================================\n",
                "if trained_model_available and history:\n",
                "    print(\"=\" * 60)\n",
                "    print(\"               TRAINING QUALITY ASSESSMENT\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Check loss convergence\n",
                "    loss_decrease = history['train_loss'][0] - history['train_loss'][-1]\n",
                "    print(f\"\\n1. Loss Convergence:\")\n",
                "    if loss_decrease > 0:\n",
                "        print(f\"   ‚úÖ Training loss decreased by {loss_decrease:.4f}\")\n",
                "    else:\n",
                "        print(f\"   ‚ùå Training loss did not decrease\")\n",
                "    \n",
                "    # Check for overfitting\n",
                "    val_loss_final = history['val_loss'][-1]\n",
                "    train_loss_final = history['train_loss'][-1]\n",
                "    print(f\"\\n2. Overfitting Check:\")\n",
                "    if val_loss_final > train_loss_final * 1.5:\n",
                "        print(f\"   ‚ö†Ô∏è Potential overfitting (Val loss >> Train loss)\")\n",
                "    else:\n",
                "        print(f\"   ‚úÖ No significant overfitting detected\")\n",
                "    \n",
                "    # Check AUC\n",
                "    print(f\"\\n3. Model Discriminative Power:\")\n",
                "    print(f\"   AUC Score: {roc_auc:.4f}\")\n",
                "    if roc_auc > 0.7:\n",
                "        print(f\"   ‚úÖ Good discriminative power\")\n",
                "    elif roc_auc > 0.5:\n",
                "        print(f\"   ‚ö†Ô∏è Moderate - consider more training or hyperparameter tuning\")\n",
                "    else:\n",
                "        print(f\"   ‚ùå Poor - model may not have learned meaningful patterns\")\n",
                "    \n",
                "    # Final F1\n",
                "    print(f\"\\n4. Final Validation Metrics:\")\n",
                "    print(f\"   Precision: {history['val_precision'][-1]:.4f}\")\n",
                "    print(f\"   Recall:    {history['val_recall'][-1]:.4f}\")\n",
                "    print(f\"   F1 Score:  {history['val_f1'][-1]:.4f}\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}