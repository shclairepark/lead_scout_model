{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Setup\n",
                "# ============================================================\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from src.tokenizer import SalesTokenizer\n",
                "from src.model.positional_encoding import PositionalEncoding\n",
                "from src.model.attention import SelfAttention, MultiHeadAttention\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"‚úÖ Imports successful\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Load Real Generated Data\n",
                "# ============================================================\n",
                "# Load the generated leads\n",
                "df = pd.read_csv('../data/leads_raw.csv')\n",
                "\n",
                "print(f\"üìä Dataset loaded: {len(df)} leads\")\n",
                "print(f\"üìà Reply rate: {df['replied'].mean():.2%}\")\n",
                "print(f\"\\nFirst 5 leads:\")\n",
                "print(df.head())\n",
                "\n",
                "print(f\"\\nData types:\")\n",
                "print(df.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: Analyze Data Distribution by Reply Status\n",
                "# ============================================================\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Reply distribution\n",
                "df['replied'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['#ff6b6b', '#51cf66'])\n",
                "axes[0, 0].set_title('Reply Distribution')\n",
                "axes[0, 0].set_xlabel('Replied')\n",
                "axes[0, 0].set_ylabel('Count')\n",
                "axes[0, 0].set_xticklabels(['No Reply', 'Replied'], rotation=0)\n",
                "\n",
                "# Months in role by reply status\n",
                "df.boxplot(column='months_in_role', by='replied', ax=axes[0, 1])\n",
                "axes[0, 1].set_title('Months in Role by Reply Status')\n",
                "axes[0, 1].set_xlabel('Replied')\n",
                "axes[0, 1].set_ylabel('Months in Role')\n",
                "\n",
                "# Funding by reply status\n",
                "replied_funding = df[df['replied'] == 1]['funding_amount']\n",
                "no_reply_funding = df[df['replied'] == 0]['funding_amount']\n",
                "\n",
                "axes[1, 0].hist([no_reply_funding, replied_funding], \n",
                "                bins=20, label=['No Reply', 'Replied'],\n",
                "                color=['#ff6b6b', '#51cf66'], alpha=0.7)\n",
                "axes[1, 0].set_title('Funding Distribution by Reply Status')\n",
                "axes[1, 0].set_xlabel('Funding Amount')\n",
                "axes[1, 0].set_ylabel('Count')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].set_xscale('log')\n",
                "\n",
                "# Own views momentum\n",
                "df['momentum'] = df['own_views_1m'] / (df['own_views_3m'] / 3 + 1)\n",
                "df.boxplot(column='momentum', by='replied', ax=axes[1, 1])\n",
                "axes[1, 1].set_title('Momentum by Reply Status')\n",
                "axes[1, 1].set_xlabel('Replied')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 4: Tokenize Sample Leads\n",
                "# ============================================================\n",
                "tokenizer = SalesTokenizer()\n",
                "\n",
                "# Select interesting examples\n",
                "print(\"=== Tokenizing Sample Leads ===\")\n",
                "\n",
                "# Example 1: High-reply lead\n",
                "high_reply_leads = df[df['replied'] == 1].head(3)\n",
                "\n",
                "# Example 2: No-reply lead\n",
                "no_reply_leads = df[df['replied'] == 0].head(3)\n",
                "\n",
                "sample_leads = pd.concat([high_reply_leads, no_reply_leads])\n",
                "\n",
                "for idx, row in sample_leads.iterrows():\n",
                "    lead_dict = row.to_dict()\n",
                "    tokens, token_ids = tokenizer.tokenize_lead(lead_dict)\n",
                "    \n",
                "    print(f\"Lead {idx} (Replied: {row['replied']}):\")\n",
                "    print(f\"  Raw: months={row['months_in_role']}, funding=${row['funding_amount']:,.0f}\")\n",
                "    print(f\"  Tokens: {tokens}\")\n",
                "    print(f\"  Token IDs: {token_ids}\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Create Embeddings for a Batch\n",
                "# ============================================================\n",
                "# Select 5 leads for attention analysis\n",
                "sample_df = df.head(5)\n",
                "\n",
                "# Tokenize all\n",
                "batch_token_ids = []\n",
                "batch_tokens_list = []\n",
                "\n",
                "for idx, row in sample_df.iterrows():\n",
                "    lead_dict = row.to_dict()\n",
                "    tokens, token_ids = tokenizer.tokenize_lead(lead_dict)\n",
                "    batch_token_ids.append(token_ids)\n",
                "    batch_tokens_list.append(tokens)\n",
                "\n",
                "# Convert to tensor\n",
                "batch_tensor = torch.tensor(batch_token_ids)  # [5, 6]\n",
                "print(f\"Batch tensor shape: {batch_tensor.shape}\")\n",
                "\n",
                "# Initialize layers\n",
                "vocab_size = len(tokenizer.vocab)\n",
                "embed_dim = 128\n",
                "\n",
                "embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
                "pos_encoder = PositionalEncoding(embed_dim, max_len=32)\n",
                "\n",
                "# Get embeddings\n",
                "embeddings = embedding_layer(batch_tensor)  # [5, 6, 128]\n",
                "embeddings = pos_encoder(embeddings)        # Add positional encoding\n",
                "\n",
                "print(f\"Embeddings shape: {embeddings.shape}\")\n",
                "print(f\"‚úÖ Created embeddings for {len(batch_token_ids)} leads\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Apply Self-Attention (Single Head)\n",
                "# ============================================================\n",
                "print(\"\\n=== Single-Head Self-Attention ===\")\n",
                "\n",
                "attention = SelfAttention(embed_dim)\n",
                "output, attention_weights = attention(embeddings)\n",
                "\n",
                "print(f\"Input shape:  {embeddings.shape}\")      # [5, 6, 128]\n",
                "print(f\"Output shape: {output.shape}\")          # [5, 6, 128] ‚Üê Same!\n",
                "print(f\"Attention weights shape: {attention_weights.shape}\")  # [5, 6, 6]\n",
                "\n",
                "# Verify shape is preserved\n",
                "assert output.shape == embeddings.shape, \"Shape should be preserved!\"\n",
                "print(\"‚úÖ Shape preserved through attention\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 7: Visualize Attention for First Lead\n",
                "# ============================================================\n",
                "# Take first lead's attention weights\n",
                "lead_idx = 0\n",
                "attn = attention_weights[lead_idx].detach().numpy()\n",
                "tokens = batch_tokens_list[lead_idx]\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(attn,\n",
                "            annot=True,\n",
                "            fmt='.3f',\n",
                "            cmap='YlOrRd',\n",
                "            xticklabels=tokens,\n",
                "            yticklabels=tokens,\n",
                "            vmin=0, vmax=1,\n",
                "            cbar_kws={'label': 'Attention Weight'})\n",
                "\n",
                "plt.title(f'Self-Attention Weights - Lead {lead_idx}\\n(Replied: {sample_df.iloc[lead_idx][\"replied\"]})')\n",
                "plt.xlabel('Attending TO (these tokens)')\n",
                "plt.ylabel('Attending FROM (these tokens)')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print interpretation\n",
                "print(f\"\\n=== Attention Pattern for Lead {lead_idx} ===\")\n",
                "print(f\"Replied: {sample_df.iloc[lead_idx]['replied']}\")\n",
                "print(f\"Tokens: {tokens}\\n\")\n",
                "\n",
                "for i, token_from in enumerate(tokens):\n",
                "    print(f\"{token_from} attends to:\")\n",
                "    weights = attn[i]\n",
                "    \n",
                "    # Sort by attention weight\n",
                "    sorted_indices = np.argsort(weights)[::-1]\n",
                "    \n",
                "    for idx in sorted_indices[:3]:  # Top 3\n",
                "        print(f\"  ‚Üí {tokens[idx]:25} {weights[idx]:.3f}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 8: Compare Replied vs Not Replied Leads\n",
                "# ============================================================\n",
                "print(\"\\n=== Comparing Attention Patterns ===\")\n",
                "\n",
                "# Find one replied and one not-replied lead\n",
                "replied_idx = sample_df[sample_df['replied'] == 1].index[0] if any(sample_df['replied'] == 1) else None\n",
                "not_replied_idx = sample_df[sample_df['replied'] == 0].index[0] if any(sample_df['replied'] == 0) else None\n",
                "\n",
                "if replied_idx is not None and not_replied_idx is not None:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "    \n",
                "    # Replied lead\n",
                "    replied_lead_pos = sample_df.index.get_loc(replied_idx)\n",
                "    attn_replied = attention_weights[replied_lead_pos].detach().numpy()\n",
                "    tokens_replied = batch_tokens_list[replied_lead_pos]\n",
                "    \n",
                "    sns.heatmap(attn_replied, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "                xticklabels=tokens_replied, yticklabels=tokens_replied,\n",
                "                vmin=0, vmax=1, ax=axes[0])\n",
                "    axes[0].set_title(f'REPLIED Lead (months={sample_df.loc[replied_idx, \"months_in_role\"]}, funding=${sample_df.loc[replied_idx, \"funding_amount\"]:,.0f})')\n",
                "    \n",
                "    # Not replied lead\n",
                "    not_replied_lead_pos = sample_df.index.get_loc(not_replied_idx)\n",
                "    attn_not_replied = attention_weights[not_replied_lead_pos].detach().numpy()\n",
                "    tokens_not_replied = batch_tokens_list[not_replied_lead_pos]\n",
                "    \n",
                "    sns.heatmap(attn_not_replied, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "                xticklabels=tokens_not_replied, yticklabels=tokens_not_replied,\n",
                "                vmin=0, vmax=1, ax=axes[1])\n",
                "    axes[1].set_title(f'NOT REPLIED Lead (months={sample_df.loc[not_replied_idx, \"months_in_role\"]}, funding=${sample_df.loc[not_replied_idx, \"funding_amount\"]:,.0f})')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Need both replied and not-replied leads in sample\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 9: Multi-Head Attention\n",
                "# ============================================================\n",
                "print(\"\\n=== Multi-Head Attention (4 heads) ===\")\n",
                "\n",
                "multi_head_attn = MultiHeadAttention(embed_dim, num_heads=4)\n",
                "mh_output, mh_attention_weights = multi_head_attn(embeddings)\n",
                "\n",
                "print(f\"Output shape: {mh_output.shape}\")  # [5, 6, 128]\n",
                "print(f\"Attention weights shape: {mh_attention_weights.shape}\")  # [5, 6, 6]\n",
                "\n",
                "# Visualize\n",
                "lead_idx = 0\n",
                "attn_mh = mh_attention_weights[lead_idx].detach().numpy()\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(attn_mh,\n",
                "            annot=True,\n",
                "            fmt='.3f',\n",
                "            cmap='YlOrRd',\n",
                "            xticklabels=batch_tokens_list[lead_idx],\n",
                "            yticklabels=batch_tokens_list[lead_idx],\n",
                "            vmin=0, vmax=1)\n",
                "\n",
                "plt.title(f'Multi-Head Attention (averaged) - Lead {lead_idx}')\n",
                "plt.xlabel('Attending TO')\n",
                "plt.ylabel('Attending FROM')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 10: Compare Single-Head vs Multi-Head\n",
                "# ============================================================\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "lead_idx = 0\n",
                "tokens = batch_tokens_list[lead_idx]\n",
                "\n",
                "# Single-head\n",
                "sh_attn = attention_weights[lead_idx].detach().numpy()\n",
                "sns.heatmap(sh_attn, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=tokens, yticklabels=tokens,\n",
                "            vmin=0, vmax=1, ax=axes[0])\n",
                "axes[0].set_title('Single-Head Attention')\n",
                "\n",
                "# Multi-head\n",
                "mh_attn = mh_attention_weights[lead_idx].detach().numpy()\n",
                "sns.heatmap(mh_attn, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=tokens, yticklabels=tokens,\n",
                "            vmin=0, vmax=1, ax=axes[1])\n",
                "axes[1].set_title('Multi-Head Attention (averaged across 4 heads)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n=== Attention Pattern Differences ===\")\n",
                "print(\"Single-head attention focuses on specific patterns\")\n",
                "print(\"Multi-head attention can learn multiple types of relationships simultaneously\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 11: Statistical Analysis\n",
                "# ============================================================\n",
                "print(\"\\n=== Attention Statistics Across Batch ===\")\n",
                "\n",
                "# Average attention each token type receives\n",
                "token_attention_stats = {}\n",
                "\n",
                "for batch_idx in range(len(batch_tokens_list)):\n",
                "    tokens = batch_tokens_list[batch_idx]\n",
                "    attn = attention_weights[batch_idx].detach().numpy()\n",
                "    \n",
                "    for token_idx, token in enumerate(tokens):\n",
                "        if token not in [\"[START]\", \"[END]\"]:  # Skip special tokens\n",
                "            # How much attention does this token receive from others?\n",
                "            received_attention = attn[:, token_idx].mean()  # Average across all queries\n",
                "            \n",
                "            if token not in token_attention_stats:\n",
                "                token_attention_stats[token] = []\n",
                "            token_attention_stats[token].append(received_attention)\n",
                "\n",
                "# Calculate averages\n",
                "print(\"\\nAverage attention received by token type:\")\n",
                "for token, attentions in sorted(token_attention_stats.items(), \n",
                "                                key=lambda x: np.mean(x[1]), \n",
                "                                reverse=True):\n",
                "    print(f\"  {token:25} ‚Üí {np.mean(attentions):.3f} (¬±{np.std(attentions):.3f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 12: Attention Patterns in Replied vs Not Replied\n",
                "# ============================================================\n",
                "print(\"\\n=== Attention Patterns by Reply Status ===\")\n",
                "\n",
                "# Process all leads in dataset (or subset for speed)\n",
                "sample_size = 100  # Process 100 leads\n",
                "sample_indices = np.random.choice(len(df), size=sample_size, replace=False)\n",
                "sample_batch_df = df.iloc[sample_indices].reset_index(drop=True)\n",
                "\n",
                "# Tokenize batch\n",
                "batch_token_ids = []\n",
                "batch_labels = []\n",
                "\n",
                "for idx, row in sample_batch_df.iterrows():\n",
                "    lead_dict = row.to_dict()\n",
                "    _, token_ids = tokenizer.tokenize_lead(lead_dict)\n",
                "    batch_token_ids.append(token_ids)\n",
                "    batch_labels.append(row['replied'])\n",
                "\n",
                "# Convert to tensor\n",
                "batch_tensor = torch.tensor(batch_token_ids)  # [100, 6]\n",
                "batch_labels = torch.tensor(batch_labels)\n",
                "\n",
                "# Get embeddings and attention\n",
                "embeddings_batch = embedding_layer(batch_tensor)\n",
                "embeddings_batch = pos_encoder(embeddings_batch)\n",
                "\n",
                "_, attention_batch = attention(embeddings_batch)  # [100, 6, 6]\n",
                "\n",
                "# Separate by reply status\n",
                "replied_mask = batch_labels == 1\n",
                "not_replied_mask = batch_labels == 0\n",
                "\n",
                "attn_replied = attention_batch[replied_mask].mean(dim=0).detach().numpy()  # Average across replied leads\n",
                "attn_not_replied = attention_batch[not_replied_mask].mean(dim=0).detach().numpy()  # Average across not-replied\n",
                "\n",
                "# Get token labels (use first lead's tokens as reference)\n",
                "reference_tokens = batch_tokens_list[0]\n",
                "\n",
                "# Plot comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "sns.heatmap(attn_replied, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=reference_tokens, yticklabels=reference_tokens,\n",
                "            vmin=0, vmax=0.5, ax=axes[0])\n",
                "axes[0].set_title(f'Average Attention: REPLIED Leads (n={replied_mask.sum()})')\n",
                "\n",
                "sns.heatmap(attn_not_replied, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=reference_tokens, yticklabels=reference_tokens,\n",
                "            vmin=0, vmax=0.5, ax=axes[1])\n",
                "axes[1].set_title(f'Average Attention: NOT REPLIED Leads (n={not_replied_mask.sum()})')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n‚úÖ Analyzed {sample_size} leads\")\n",
                "print(f\"   - Replied: {replied_mask.sum()}\")\n",
                "print(f\"   - Not replied: {not_replied_mask.sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 13: Specific Pattern Analysis\n",
                "# ============================================================\n",
                "print(\"\\n=== Key Attention Pattern Analysis ===\")\n",
                "\n",
                "# Analyze specific token pair interactions\n",
                "# Position 1 (TENURE) ‚Üí Position 2 (FUNDING) attention\n",
                "\n",
                "tenure_to_funding_replied = []\n",
                "tenure_to_funding_not_replied = []\n",
                "\n",
                "for batch_idx in range(len(batch_labels)):\n",
                "    attn = attention_batch[batch_idx].detach().numpy()\n",
                "    \n",
                "    # Attention from position 1 (TENURE) to position 2 (FUNDING)\n",
                "    tenure_funding_attn = attn[1, 2]\n",
                "    \n",
                "    if batch_labels[batch_idx] == 1:\n",
                "        tenure_to_funding_replied.append(tenure_funding_attn)\n",
                "    else:\n",
                "        tenure_to_funding_not_replied.append(tenure_funding_attn)\n",
                "\n",
                "print(f\"\\nTENURE ‚Üí FUNDING attention:\")\n",
                "print(f\"  Replied leads:     {np.mean(tenure_to_funding_replied):.3f} (¬±{np.std(tenure_to_funding_replied):.3f})\")\n",
                "print(f\"  Not replied leads: {np.mean(tenure_to_funding_not_replied):.3f} (¬±{np.std(tenure_to_funding_not_replied):.3f})\")\n",
                "\n",
                "# Statistical test\n",
                "from scipy import stats\n",
                "t_stat, p_value = stats.ttest_ind(tenure_to_funding_replied, tenure_to_funding_not_replied)\n",
                "print(f\"  t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}\")\n",
                "\n",
                "if p_value < 0.05:\n",
                "    print(\"  ‚úÖ Significant difference! Model learned meaningful attention pattern\")\n",
                "else:\n",
                "    print(\"  ‚ö†Ô∏è No significant difference (model not trained yet)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 14: Multi-Head Attention Comparison\n",
                "# ============================================================\n",
                "print(\"\\n=== Multi-Head Attention Analysis ===\")\n",
                "\n",
                "multi_head_attn = MultiHeadAttention(embed_dim, num_heads=4)\n",
                "mh_output, mh_attn_weights = multi_head_attn(embeddings)\n",
                "\n",
                "print(f\"Multi-head output shape: {mh_output.shape}\")\n",
                "\n",
                "# Compare first lead\n",
                "lead_idx = 0\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "# Single-head\n",
                "sh_attn = attention_weights[lead_idx].detach().numpy()\n",
                "sns.heatmap(sh_attn, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=batch_tokens_list[lead_idx],\n",
                "            yticklabels=batch_tokens_list[lead_idx],\n",
                "            vmin=0, vmax=1, ax=axes[0])\n",
                "axes[0].set_title('Single-Head Attention')\n",
                "\n",
                "# Multi-head\n",
                "mh_attn = mh_attn_weights[lead_idx].detach().numpy()\n",
                "sns.heatmap(mh_attn, annot=True, fmt='.3f', cmap='YlOrRd',\n",
                "            xticklabels=batch_tokens_list[lead_idx],\n",
                "            yticklabels=batch_tokens_list[lead_idx],\n",
                "            vmin=0, vmax=1, ax=axes[1])\n",
                "axes[1].set_title('Multi-Head Attention (averaged)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"üìä Tested on {len(sample_batch_df)} real leads from your dataset\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}